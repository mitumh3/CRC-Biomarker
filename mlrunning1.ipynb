{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f7bb21",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.013252,
     "end_time": "2022-06-19T10:40:24.738134",
     "exception": false,
     "start_time": "2022-06-19T10:40:24.724882",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43b8766",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T10:40:24.765305Z",
     "iopub.status.busy": "2022-06-19T10:40:24.764376Z",
     "iopub.status.idle": "2022-06-19T10:40:36.754010Z",
     "shell.execute_reply": "2022-06-19T10:40:36.752672Z"
    },
    "papermill": {
     "duration": 12.005938,
     "end_time": "2022-06-19T10:40:36.756744",
     "exception": false,
     "start_time": "2022-06-19T10:40:24.750806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import git\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "oauthkey = UserSecretsClient().get_secret('oauthkey')\n",
    "print(oauthkey)\n",
    "from git import Repo\n",
    "a = Repo.clone_from(\"https://\"+oauthkey+\"@github.com/mitumh3/CRC-FeatureSelection.git\", \"/kaggle/working/feature\")\n",
    "b = Repo.clone_from(\"https://\"+oauthkey+\"@github.com/mitumh3/CRC-FS-output.git\", \"/kaggle/working/output\")\n",
    "c = Repo.clone_from(\"https://\"+oauthkey+\"@github.com/mitumh3/CRC-usedF.git\", \"/kaggle/working/used\")\n",
    "!git config --global user.email \"truongngocminh0505@gmail.com\"\n",
    "!git config --global user.name \"mitumh3\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d59e98f",
   "metadata": {
    "papermill": {
     "duration": 0.012546,
     "end_time": "2022-06-19T10:40:36.781688",
     "exception": false,
     "start_time": "2022-06-19T10:40:36.769142",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# . Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67c17df",
   "metadata": {
    "papermill": {
     "duration": 0.012034,
     "end_time": "2022-06-19T10:40:36.806552",
     "exception": false,
     "start_time": "2022-06-19T10:40:36.794518",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    ">>>Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26d11ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T10:40:36.832754Z",
     "iopub.status.busy": "2022-06-19T10:40:36.832349Z",
     "iopub.status.idle": "2022-06-19T10:40:41.812759Z",
     "shell.execute_reply": "2022-06-19T10:40:41.811481Z"
    },
    "papermill": {
     "duration": 4.996669,
     "end_time": "2022-06-19T10:40:41.815355",
     "exception": false,
     "start_time": "2022-06-19T10:40:36.818686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Can get the functions\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.svm import SVC\n",
    "!pip install scikit-survival --no-dependencies\n",
    "from sksurv.compare import compare_survival\n",
    "# Ignore wanring\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ee591c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T10:40:41.843838Z",
     "iopub.status.busy": "2022-06-19T10:40:41.843391Z",
     "iopub.status.idle": "2022-06-19T10:40:59.496416Z",
     "shell.execute_reply": "2022-06-19T10:40:59.495023Z"
    },
    "papermill": {
     "duration": 17.669984,
     "end_time": "2022-06-19T10:40:59.498876",
     "exception": false,
     "start_time": "2022-06-19T10:40:41.828892",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install pyblaze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f028342",
   "metadata": {
    "papermill": {
     "duration": 0.01483,
     "end_time": "2022-06-19T10:40:59.528695",
     "exception": false,
     "start_time": "2022-06-19T10:40:59.513865",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    ">>>Basic process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e8b72d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T10:40:59.560098Z",
     "iopub.status.busy": "2022-06-19T10:40:59.559633Z",
     "iopub.status.idle": "2022-06-19T10:40:59.568186Z",
     "shell.execute_reply": "2022-06-19T10:40:59.567239Z"
    },
    "papermill": {
     "duration": 0.026708,
     "end_time": "2022-06-19T10:40:59.570280",
     "exception": false,
     "start_time": "2022-06-19T10:40:59.543572",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Basic process\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import statistics as s\n",
    "import scipy.stats as stats\n",
    "import os\n",
    "import sys\n",
    "import ast\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755c483e",
   "metadata": {
    "papermill": {
     "duration": 0.014615,
     "end_time": "2022-06-19T10:40:59.600158",
     "exception": false,
     "start_time": "2022-06-19T10:40:59.585543",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    ">>>Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c937a6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T10:40:59.631356Z",
     "iopub.status.busy": "2022-06-19T10:40:59.630927Z",
     "iopub.status.idle": "2022-06-19T10:40:59.635565Z",
     "shell.execute_reply": "2022-06-19T10:40:59.634597Z"
    },
    "papermill": {
     "duration": 0.022451,
     "end_time": "2022-06-19T10:40:59.637535",
     "exception": false,
     "start_time": "2022-06-19T10:40:59.615084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Method for normalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c29581",
   "metadata": {
    "papermill": {
     "duration": 0.014247,
     "end_time": "2022-06-19T10:40:59.666401",
     "exception": false,
     "start_time": "2022-06-19T10:40:59.652154",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    ">>>Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520652d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-18T15:42:06.223366Z",
     "iopub.status.busy": "2022-06-18T15:42:06.222926Z",
     "iopub.status.idle": "2022-06-18T15:42:06.830568Z",
     "shell.execute_reply": "2022-06-18T15:42:06.829574Z",
     "shell.execute_reply.started": "2022-06-18T15:42:06.223269Z"
    },
    "papermill": {
     "duration": 0.014147,
     "end_time": "2022-06-19T10:40:59.695002",
     "exception": false,
     "start_time": "2022-06-19T10:40:59.680855",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08d6520",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T10:40:59.726070Z",
     "iopub.status.busy": "2022-06-19T10:40:59.725420Z",
     "iopub.status.idle": "2022-06-19T10:40:59.729155Z",
     "shell.execute_reply": "2022-06-19T10:40:59.728531Z"
    },
    "papermill": {
     "duration": 0.021589,
     "end_time": "2022-06-19T10:40:59.731149",
     "exception": false,
     "start_time": "2022-06-19T10:40:59.709560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Can not get the functions\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a869fbfd",
   "metadata": {
    "papermill": {
     "duration": 0.014278,
     "end_time": "2022-06-19T10:40:59.760329",
     "exception": false,
     "start_time": "2022-06-19T10:40:59.746051",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    ">>>Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d083de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T10:40:59.791559Z",
     "iopub.status.busy": "2022-06-19T10:40:59.790565Z",
     "iopub.status.idle": "2022-06-19T10:40:59.794786Z",
     "shell.execute_reply": "2022-06-19T10:40:59.794081Z"
    },
    "papermill": {
     "duration": 0.021999,
     "end_time": "2022-06-19T10:40:59.796850",
     "exception": false,
     "start_time": "2022-06-19T10:40:59.774851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Parameters optimization\n",
    "from sklearn.model_selection import GridSearchCV "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb0db91",
   "metadata": {
    "papermill": {
     "duration": 0.014313,
     "end_time": "2022-06-19T10:40:59.825958",
     "exception": false,
     "start_time": "2022-06-19T10:40:59.811645",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    ">>>Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbbc475",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T10:40:59.857323Z",
     "iopub.status.busy": "2022-06-19T10:40:59.856888Z",
     "iopub.status.idle": "2022-06-19T10:40:59.861219Z",
     "shell.execute_reply": "2022-06-19T10:40:59.860304Z"
    },
    "papermill": {
     "duration": 0.022757,
     "end_time": "2022-06-19T10:40:59.863415",
     "exception": false,
     "start_time": "2022-06-19T10:40:59.840658",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Performance\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ddd721",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-18T15:42:49.271059Z",
     "iopub.status.busy": "2022-06-18T15:42:49.270662Z",
     "iopub.status.idle": "2022-06-18T15:42:52.406021Z",
     "shell.execute_reply": "2022-06-18T15:42:52.404411Z",
     "shell.execute_reply.started": "2022-06-18T15:42:49.270996Z"
    },
    "papermill": {
     "duration": 0.01488,
     "end_time": "2022-06-19T10:40:59.893331",
     "exception": false,
     "start_time": "2022-06-19T10:40:59.878451",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ef7856",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T10:40:59.924492Z",
     "iopub.status.busy": "2022-06-19T10:40:59.923926Z",
     "iopub.status.idle": "2022-06-19T10:41:01.732030Z",
     "shell.execute_reply": "2022-06-19T10:41:01.730995Z"
    },
    "papermill": {
     "duration": 1.826468,
     "end_time": "2022-06-19T10:41:01.734503",
     "exception": false,
     "start_time": "2022-06-19T10:40:59.908035",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run parralell\n",
    "import pyblaze.multiprocessing as xmp\n",
    "import multiprocessing as mp\n",
    "mp.set_start_method('fork', force = True)\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f915b137",
   "metadata": {
    "papermill": {
     "duration": 0.01467,
     "end_time": "2022-06-19T10:41:01.763797",
     "exception": false,
     "start_time": "2022-06-19T10:41:01.749127",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# .Process Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c20f5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T10:41:01.795757Z",
     "iopub.status.busy": "2022-06-19T10:41:01.795042Z",
     "iopub.status.idle": "2022-06-19T10:41:01.819279Z",
     "shell.execute_reply": "2022-06-19T10:41:01.818351Z"
    },
    "papermill": {
     "duration": 0.042933,
     "end_time": "2022-06-19T10:41:01.821707",
     "exception": false,
     "start_time": "2022-06-19T10:41:01.778774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def valid(DTS, save):\n",
    "    print(\"\\nFound folder: \\n\",DTS)\n",
    "    clin= pd.read_csv(\n",
    "        \"/kaggle/input/colorectalcancer/\" + DTS + \"_clinicaldata.txt\", sep=\"\\t\", \n",
    "        index_col=\"!Sample_geo_accession\")\n",
    "    genes= pd.read_csv(\n",
    "        \"/kaggle/input/colorectalcancer/\" + DTS + \"_genes.txt\", sep=\"\\t\")\n",
    "\n",
    "    clin = clin.rename(columns={\"status\":\"target\"})\n",
    "    genes = genes.T\n",
    "    # print(type(genes))\n",
    "    genes = genes.rename(columns=genes.iloc[0]).drop(genes.index[0])\n",
    "    # data = data.join(clin.filter(regex=(target)))\n",
    "    # print(\"\\nHEADINGS\\n\" \n",
    "    #     , clin.head())\n",
    "    # print(\"\\nSUMMARY\\n\"\n",
    "    #     , clin.describe(include=['object']))\n",
    "    # print(\"\\nCOLUMN SUMMARY\\n\", clin['target'].value_counts(normalize=True))\n",
    "    # print(clin['target'].value_counts(normalize=True).items)\n",
    "\n",
    "            # except: \n",
    "            #     print(\"No found folder or file\")\n",
    "            #     pass\n",
    "\n",
    "    def classification_train(df):\n",
    "        if df[\"target\"] == \" colorectal cancer\":\n",
    "            return 1\n",
    "        elif (df[\"target\"] == \" normal\") | (df[\"target\"] == \" non-Cancer\"):\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "    def classification_DFS(df):\n",
    "        if df[\"DFS\"] == 1:\n",
    "            return True\n",
    "        elif df[\"DFS\"] == 0:\n",
    "            return False\n",
    "        else:\n",
    "            return \"NA\"\n",
    "\n",
    "    clin[\"target\"] = clin.apply(classification_train, axis=1)\n",
    "    clin[\"DFS\"] = clin.apply(classification_DFS, axis=1)\n",
    "    # print(\"\\nTARGET in CLIN\\n\" \n",
    "    #     , clin[\"target\"])\n",
    "    print(pd.DataFrame(clin['target'].value_counts(normalize=True)).iloc[0, 0])\n",
    "    print(clin['target'].count())\n",
    "    # AUC(clin = clin, genes = genes, output = DTS, method = \"Standard\")\n",
    "    data = genes.join(clin.filter(regex=(\"target\")))\n",
    "    data = data.join(clin.filter(regex=(\"DFS\")))\n",
    "    data = data.join(clin.filter(regex=(\"stage\")))   \n",
    "    print(data)\n",
    "    if save == True:\n",
    "        dir = os.path.join(\"/content/drive/MyDrive/THESIS/machine-learning/processed_data\",DTS)\n",
    "        print(dir)\n",
    "        if not os.path.exists(dir):\n",
    "            os.mkdir(dir)\n",
    "        print(\"\\nSaving...\\n\")\n",
    "        data.to_csv(dir +\"/processed_data_\"+DTS +\".txt\", sep=\"\\t\")\n",
    "        print(\"\\nDone!\\n\")\n",
    "    elif save == False:\n",
    "        pass\n",
    "    return(data)\n",
    "\n",
    "def train(DTS, save):\n",
    "    print(\"\\nFound folder: \\n\",DTS)\n",
    "    clin= pd.read_csv(\n",
    "        \"/kaggle/input/colorectalcancer/\" + DTS + \"_clinicaldata.txt\", sep=\"\\t\", \n",
    "        index_col=\"!Sample_geo_accession\")\n",
    "    genes= pd.read_csv(\n",
    "        \"/kaggle/input/colorectalcancer/\" + DTS + \"_genes.txt\", sep=\"\\t\")\n",
    "\n",
    "    clin = clin.rename(columns={\"status\":\"target\"})\n",
    "    genes = genes.T\n",
    "    # print(type(genes))\n",
    "    genes = genes.rename(columns=genes.iloc[0]).drop(genes.index[0])\n",
    "    # data = data.join(clin.filter(regex=(target)))\n",
    "    # print(\"\\nHEADINGS\\n\" \n",
    "    #     , clin.head())\n",
    "    # print(\"\\nSUMMARY\\n\"\n",
    "    #     , clin.describe(include=['object']))\n",
    "    # print(\"\\nCOLUMN SUMMARY\\n\", clin['target'].value_counts(normalize=True))\n",
    "    # print(clin['target'].value_counts(normalize=True).items)\n",
    "\n",
    "            # except: \n",
    "            #     print(\"No found folder or file\")\n",
    "            #     pass\n",
    "\n",
    "    def classification_train(df):\n",
    "        if df[\"target\"] == \" colorectal cancer\":\n",
    "            return 1\n",
    "        elif (df[\"target\"] == \" normal\") | (df[\"target\"] == \" non-Cancer\"):\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "    clin[\"target\"] = clin.apply(classification_train, axis=1)\n",
    "    # print(\"\\nTARGET in CLIN\\n\" \n",
    "    #     , clin[\"target\"])\n",
    "    print(pd.DataFrame(clin['target'].value_counts(normalize=True)).iloc[0, 0])\n",
    "    print(clin['target'].count())\n",
    "    # AUC(clin = clin, genes = genes, output = DTS, method = \"Standard\")\n",
    "    data = genes.join(clin.filter(regex=(\"target\")))\n",
    "    print(data)\n",
    "    if save == True:\n",
    "        dir = os.path.join(\"/content/drive/MyDrive/THESIS/machine-learning/processed_data/processed_data\",DTS)\n",
    "        print(dir)\n",
    "        if not os.path.exists(dir):\n",
    "            os.mkdir(dir)\n",
    "        print(\"\\nSaving...\\n\")\n",
    "        data.to_csv(dir +\"/processed_data_\"+DTS +\".txt\", sep=\"\\t\")\n",
    "        print(\"\\nDone!\\n\")\n",
    "    elif save == False:\n",
    "        pass\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c582e4",
   "metadata": {
    "papermill": {
     "duration": 0.014447,
     "end_time": "2022-06-19T10:41:01.850711",
     "exception": false,
     "start_time": "2022-06-19T10:41:01.836264",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# .Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d832e32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T10:41:01.882327Z",
     "iopub.status.busy": "2022-06-19T10:41:01.881692Z",
     "iopub.status.idle": "2022-06-19T10:41:01.922955Z",
     "shell.execute_reply": "2022-06-19T10:41:01.922121Z"
    },
    "papermill": {
     "duration": 0.059801,
     "end_time": "2022-06-19T10:41:01.925173",
     "exception": false,
     "start_time": "2022-06-19T10:41:01.865372",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import functools\n",
    "import numpy as np\n",
    "import torch.multiprocessing as mp\n",
    "from pyblaze.utils.stdmp import terminate\n",
    "from tqdm import tqdm\n",
    "class Vectorizer:\n",
    "    \"\"\"\n",
    "    The Vectorizer class ought to be used in cases where a result tensor of size N is filled with\n",
    "    values computed in some complex way. The computation of these N computations can then be\n",
    "    parallelized over multiple processes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, worker_func, worker_init=None, callback_func=None, num_workers=-1, **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes a new vectorizer.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        worker_func: callable\n",
    "            The function which receives as input an item of the input to process and outputs a\n",
    "            value which ought to be returned.\n",
    "        worker_init: callable, default: None\n",
    "            The function receives as input the rank of the worker (i.e. every time this function is\n",
    "            called, it is called with a different integer as parameter). Its return values are\n",
    "            passed as *last* parameters to `worker_func` upon every invocation.\n",
    "        callback_func: callable, default: None\n",
    "            A function to call after every item has been processed. Must not need to be a free\n",
    "            function as it is called on the main thread.\n",
    "        num_workers: int, default: -1\n",
    "            The number of processes to use. If set to -1, it defaults to the number of available\n",
    "            cores. If set to 0, everything is executed on the main thread.\n",
    "        kwargs: keyword arguments\n",
    "            Additional arguments passed to the worker initialization function.\n",
    "        \"\"\"\n",
    "        self.num_workers = os.cpu_count() if num_workers == -1 else num_workers\n",
    "        self.worker_func = worker_func\n",
    "        self.worker_init = worker_init\n",
    "        self.callback_func = callback_func\n",
    "        self.init_kwargs = kwargs\n",
    "        self._shutdown_fn = None\n",
    "\n",
    "    def process(self, items, *args):\n",
    "        \"\"\"\n",
    "        Uses the vectorizer's worker function in order to process all items in parallel.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        items: list of object or iterable of object\n",
    "            The items to be processed by the workers. If given as iterable only (i.e. it does not\n",
    "            support index access), the performance might suffer slightly due to an increased number\n",
    "            of synchronizations.\n",
    "        args: variadic arguments\n",
    "            Additional arguments passed directly to the worker function.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list of object\n",
    "            The output generated by the worker function for each of the input items.\n",
    "        \"\"\"\n",
    "        if self.num_workers == 0: # execute sequentially\n",
    "            result = []\n",
    "            init = _init_if_needed(self.worker_init, 0, **self.init_kwargs)\n",
    "            all_args = _combine_args(args, init)\n",
    "            for item in items:\n",
    "                result.append(self.worker_func(item, *all_args))\n",
    "            return result\n",
    "\n",
    "        process_batches = hasattr(items, '__getitem__')\n",
    "\n",
    "        if process_batches:\n",
    "            result = self._process_batches(items, *args)\n",
    "        else:\n",
    "            result = self._process_consumers(items, *args)\n",
    "\n",
    "        self._shutdown_fn()\n",
    "        self._shutdown_fn = None\n",
    "        return result\n",
    "\n",
    "    def _process_batches(self, items, *args):\n",
    "        num_items = len(items)\n",
    "\n",
    "        splits = np.array_split(np.arange(num_items), self.num_workers)\n",
    "        splits = [0] + [a[-1] + 1 for a in splits]\n",
    "\n",
    "        result = []\n",
    "\n",
    "        processes = []\n",
    "        queues = []\n",
    "        done = mp.Event()\n",
    "        if self.callback_func is not None:\n",
    "            tick_queue = mp.Queue()\n",
    "        else:\n",
    "            tick_queue = None\n",
    "        \n",
    "        for i in range(self.num_workers):\n",
    "            queue = mp.Queue()\n",
    "            process = mp.Process(\n",
    "                target=_batch_worker,\n",
    "                args=(\n",
    "                    queue, done, tick_queue, i,\n",
    "                    self.worker_func, self.worker_init, self.init_kwargs,\n",
    "                    items[splits[i]:splits[i+1]], *args\n",
    "                )\n",
    "            )\n",
    "            process.daemon = True\n",
    "            process.start()\n",
    "            \n",
    "            processes.append(process)\n",
    "            queues.append(queue)\n",
    "\n",
    "        self._shutdown_fn = functools.partial(\n",
    "            self._shutdown_batches, processes, done\n",
    "        )\n",
    "\n",
    "        if self.callback_func is not None:\n",
    "            for _ in range(num_items):\n",
    "                tick_queue.get()\n",
    "                self.callback_func()\n",
    "\n",
    "        for i, q in enumerate(queues):\n",
    "            result.extend(q.get())\n",
    "            q.close()\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _shutdown_batches(self, processes, done):\n",
    "        done.set()\n",
    "        terminate(*processes)\n",
    "\n",
    "    def _process_consumers(self, items, *args):\n",
    "        result = []\n",
    "\n",
    "        processes = []\n",
    "        push_queue = mp.Queue()\n",
    "        pull_queue = mp.Queue()\n",
    "\n",
    "        for i in range(self.num_workers):\n",
    "            process = mp.Process(\n",
    "                target=_consumer_worker,\n",
    "                args=(push_queue, pull_queue, i,\n",
    "                      self.worker_func, self.worker_init, self.init_kwargs,\n",
    "                      *args)\n",
    "            )\n",
    "            process.daemon = True\n",
    "            process.start()\n",
    "            print(\"\\nProcessor \", i, \"...\\n\")\n",
    "            processes.append(process)\n",
    "\n",
    "        self._shutdown_fn = functools.partial(\n",
    "            self._shutdown_consumers, processes, pull_queue, push_queue\n",
    "        )\n",
    "\n",
    "        iterator = iter(items)\n",
    "        index = 0\n",
    "        expect = 0\n",
    "        try:\n",
    "            for _ in range(self.num_workers):\n",
    "                item = next(iterator)\n",
    "                expect += 1\n",
    "                push_queue.cancel_join_thread()\n",
    "                push_queue.put((index, item))\n",
    "                index += 1\n",
    "\n",
    "            while True:\n",
    "                result.append(pull_queue.get())\n",
    "                if self.callback_func is not None:\n",
    "                    self.callback_func()\n",
    "                expect -= 1\n",
    "                item = next(iterator)\n",
    "                expect += 1\n",
    "                push_queue.cancel_join_thread()\n",
    "                push_queue.put((index, item))\n",
    "                index += 1\n",
    "\n",
    "        except StopIteration:\n",
    "            for _ in range(expect):\n",
    "                result.append(pull_queue.get())\n",
    "                if self.callback_func is not None:\n",
    "                    self.callback_func()\n",
    "\n",
    "        return [r[1] for r in sorted(result, key=lambda r: r[0])]\n",
    "\n",
    "    def _shutdown_consumers(self, processes, pull_queue, push_queue):\n",
    "        pull_queue.close()\n",
    "\n",
    "        for _ in range(len(processes)):\n",
    "            push_queue.cancel_join_thread()\n",
    "            push_queue.put(None)\n",
    "\n",
    "        push_queue.close()\n",
    "\n",
    "        terminate(*processes)\n",
    "\n",
    "    def __del__(self):\n",
    "        if self._shutdown_fn is not None:\n",
    "            self._shutdown_fn()\n",
    "\n",
    "\n",
    "def _batch_worker(push_queue, done, tick_queue, rank,\n",
    "                  worker_func, worker_init, init_kwargs, items, *args):\n",
    "    init = _init_if_needed(worker_init, rank, **init_kwargs)\n",
    "    all_args = _combine_args(args, init)\n",
    "    result = []\n",
    "    if rank == 1 :\n",
    "        pbar = tqdm(total = len(items), desc = \">>Processor \" + str(rank))\n",
    "    else: pbar = tqdm(total = len(items), desc = \">>Processor \" + str(rank), disable = True)\n",
    "    for item in items:\n",
    "        pbar.update(1)\n",
    "        result.append(worker_func(item, *all_args))\n",
    "        if tick_queue is not None:\n",
    "            tick_queue.cancel_join_thread()\n",
    "            tick_queue.put(None)\n",
    "    push_queue.cancel_join_thread()\n",
    "    push_queue.put(result)\n",
    "    done.wait()\n",
    "\n",
    "\n",
    "def _consumer_worker(pull_queue, push_queue, rank, worker_func, worker_init, init_kwargs, *args):\n",
    "\n",
    "    init = _init_if_needed(worker_init, rank, **init_kwargs)\n",
    "    all_args = _combine_args(args, init)\n",
    "\n",
    "    while True:\n",
    "        item = pull_queue.get()\n",
    "        if item is None:\n",
    "            break\n",
    "\n",
    "        idx, item = item\n",
    "        result = worker_func(item, *all_args)\n",
    "\n",
    "        push_queue.cancel_join_thread()\n",
    "        push_queue.put((idx, result))\n",
    "\n",
    "\n",
    "def _init_if_needed(init, rank, **kwargs):\n",
    "    if init is None:\n",
    "        return None\n",
    "    return init(rank, **kwargs)\n",
    "\n",
    "\n",
    "def _combine_args(a, b):\n",
    "    if b is None:\n",
    "        return a\n",
    "    if isinstance(b, tuple):\n",
    "        return a + b\n",
    "    return a + (b,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8468aff3",
   "metadata": {
    "papermill": {
     "duration": 0.014433,
     "end_time": "2022-06-19T10:41:01.954097",
     "exception": false,
     "start_time": "2022-06-19T10:41:01.939664",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# .Choose DTS and path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907be181",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T10:41:01.985452Z",
     "iopub.status.busy": "2022-06-19T10:41:01.984802Z",
     "iopub.status.idle": "2022-06-19T10:41:01.990314Z",
     "shell.execute_reply": "2022-06-19T10:41:01.989459Z"
    },
    "papermill": {
     "duration": 0.02362,
     "end_time": "2022-06-19T10:41:01.992396",
     "exception": false,
     "start_time": "2022-06-19T10:41:01.968776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainDTS=\"GSE164191\"\n",
    "validationDTS=\"GSE39582\"\n",
    "suffix = \"\"\n",
    "path=\"/kaggle/working/output\"\n",
    "dts = {\"train\":trainDTS, \"validation\":validationDTS}\n",
    "ml = \"ml3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a360d9",
   "metadata": {
    "papermill": {
     "duration": 0.014854,
     "end_time": "2022-06-19T10:41:02.021970",
     "exception": false,
     "start_time": "2022-06-19T10:41:02.007116",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# .Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ef04c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T10:41:02.054008Z",
     "iopub.status.busy": "2022-06-19T10:41:02.053462Z",
     "iopub.status.idle": "2022-06-19T10:41:09.063260Z",
     "shell.execute_reply": "2022-06-19T10:41:09.062090Z"
    },
    "papermill": {
     "duration": 7.028621,
     "end_time": "2022-06-19T10:41:09.065916",
     "exception": false,
     "start_time": "2022-06-19T10:41:02.037295",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = train(dts[\"train\"], save = False).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44158578",
   "metadata": {
    "papermill": {
     "duration": 0.015134,
     "end_time": "2022-06-19T10:41:09.097078",
     "exception": false,
     "start_time": "2022-06-19T10:41:09.081944",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# .Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878868df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T10:41:09.130241Z",
     "iopub.status.busy": "2022-06-19T10:41:09.129828Z",
     "iopub.status.idle": "2022-06-19T10:41:53.452252Z",
     "shell.execute_reply": "2022-06-19T10:41:53.450977Z"
    },
    "papermill": {
     "duration": 44.342351,
     "end_time": "2022-06-19T10:41:53.455177",
     "exception": false,
     "start_time": "2022-06-19T10:41:09.112826",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "validation = valid(dts[\"validation\"], save = False).dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b95910",
   "metadata": {
    "papermill": {
     "duration": 0.015386,
     "end_time": "2022-06-19T10:41:53.485870",
     "exception": false,
     "start_time": "2022-06-19T10:41:53.470484",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# .Choose characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73b5726",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T10:41:53.519038Z",
     "iopub.status.busy": "2022-06-19T10:41:53.518606Z",
     "iopub.status.idle": "2022-06-19T10:41:53.524633Z",
     "shell.execute_reply": "2022-06-19T10:41:53.523907Z"
    },
    "papermill": {
     "duration": 0.025401,
     "end_time": "2022-06-19T10:41:53.526615",
     "exception": false,
     "start_time": "2022-06-19T10:41:53.501214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# top features with highest score\n",
    "number_of_features = 200\n",
    "# The test size you want to split from train_test rang from (0:1), for large data chooose 0.1, smaller try 0.2-0.4\n",
    "test_size = 0.5\n",
    "# Method for scale data, if equal to Standard it will be z-score normalization else will be MinMaxScaler\n",
    "method = \"Standard\"\n",
    "# num_folds= number of fold validation which mean it will validation on the training data by 5 subset training, should be 5 or 10 if data is larger\n",
    "number_of_folds = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea09996d",
   "metadata": {
    "papermill": {
     "duration": 0.014915,
     "end_time": "2022-06-19T10:41:53.556663",
     "exception": false,
     "start_time": "2022-06-19T10:41:53.541748",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    ">>>*Combination Function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb20fc52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T10:41:53.589778Z",
     "iopub.status.busy": "2022-06-19T10:41:53.589351Z",
     "iopub.status.idle": "2022-06-19T10:41:53.594872Z",
     "shell.execute_reply": "2022-06-19T10:41:53.593838Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.025182,
     "end_time": "2022-06-19T10:41:53.597124",
     "exception": false,
     "start_time": "2022-06-19T10:41:53.571942",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Section 3: Making combination inputs\n",
    "def combinations(features, combinations):\n",
    "    features = list(itertools.combinations(features, combinations))\n",
    "    return features\n",
    "\n",
    "# Model name\n",
    "def model_name(selected_features):\n",
    "    return ' '.join(selected_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9172f190",
   "metadata": {
    "papermill": {
     "duration": 0.014892,
     "end_time": "2022-06-19T10:41:53.627049",
     "exception": false,
     "start_time": "2022-06-19T10:41:53.612157",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    ">>>*Parameter tuning*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be2399d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T10:41:53.659386Z",
     "iopub.status.busy": "2022-06-19T10:41:53.659005Z",
     "iopub.status.idle": "2022-06-19T10:41:53.684901Z",
     "shell.execute_reply": "2022-06-19T10:41:53.684140Z"
    },
    "papermill": {
     "duration": 0.044879,
     "end_time": "2022-06-19T10:41:53.687216",
     "exception": false,
     "start_time": "2022-06-19T10:41:53.642337",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parameter_tuning(value, path):\n",
    "    X_train, X_test, X_valid = value[0], value[1], value[2]\n",
    "    y_train, y_test, y_valid = value[3], value[4], value[5]\n",
    "    num_folds, feature_name = value[6], value[7]\n",
    "    y_valid2 = value[8]\n",
    "    y_dfs = value[9]\n",
    "    # Create models\n",
    "    # Model 4\n",
    "    log_reg_params = dict(class_weight=['balanced', None], solver=[\n",
    "                          'newton-cg', 'lbfgs', 'liblinear', 'sag'], fit_intercept=[True, False])\n",
    "    # # Create the model\n",
    "    params = [\n",
    "         log_reg_params]\n",
    "    # classifiers to test\n",
    "    classifiers = [\n",
    "        LogisticRegression()]\n",
    "\n",
    "    names = [\n",
    "        'LogisticRegression']\n",
    "    models = dict(zip(names, zip(classifiers, params)))\n",
    "    # print(num_folds, 'fold cross-validation is used')\n",
    "    results_train_train = []\n",
    "    results_train_test = []\n",
    "    results_train_valid = []\n",
    "    # dataframe to store intermediate results\n",
    "    for name, clf_and_params in models.items():\n",
    "        clf, clf_params = clf_and_params\n",
    "        # Handling error when the features is not come up with models\n",
    "        try:\n",
    "            # print('Computing GridSearch on {} '.format(name))\n",
    "            clf, clf_params = clf_and_params\n",
    "            # Parameter and training\n",
    "            grid_clf = GridSearchCV(\n",
    "                estimator=clf, param_grid=clf_params, cv=num_folds, refit= \"accuracy\", scoring = performance)\n",
    "            grid_clf = grid_clf.fit(X_train, y_train)\n",
    "            # Train\n",
    "            perform_train = performance(grid_clf, X_train, y_train)\n",
    "\n",
    "            # Testing\n",
    "            perform_test = performance(grid_clf, X_test, y_test)\n",
    "            # Validation\n",
    "            perform_valid = performance(grid_clf, X_valid, y_valid)\n",
    "\n",
    "            cv_scores = cross_val_score(clf, X_train, y_train, cv=num_folds)\n",
    "            coef = grid_clf.best_estimator_.coef_\n",
    "            intercept = grid_clf.best_estimator_.intercept_\n",
    "            # Performance\n",
    "            perform_train.update({\"cv\": np.mean(cv_scores), \"name\": feature_name,\n",
    "                                \"model_name\": name, \"best_params\": grid_clf.best_params_\n",
    "                                , \"coef\": coef, \"intercept\": intercept\n",
    "                                })\n",
    "            perform_valid.update({\"cv\": np.mean(cv_scores), \"name\": feature_name,\n",
    "                                \"model_name\": name, \"best_params\": grid_clf.best_params_\n",
    "                                , \"coef\": coef, \"intercept\": intercept\n",
    "                                })\n",
    "            perform_test.update({\"cv\": np.mean(cv_scores), \"name\": feature_name,\n",
    "                                \"model_name\": name, \"best_params\": grid_clf.best_params_\n",
    "                                , \"coef\": coef, \"intercept\": intercept\n",
    "                                })\n",
    "\n",
    "            # Results\n",
    "            results_train_train.append(perform_train)\n",
    "            results_train_test.append(perform_test)\n",
    "            results_train_valid.append(perform_valid)\n",
    "            \n",
    "        except:\n",
    "            print(\"An exception occurred\")\n",
    "            pass\n",
    "    e = math.e\n",
    "    X = 1/(1 + pow(e,-np.sum(X_valid*coef, axis = 1) + intercept))\n",
    "    stage = y_valid2\n",
    "    data = pd.DataFrame({\"gene\": X, \"stage\": stage})\n",
    "    aa = data[data[\"stage\"] == 1]\n",
    "    bb = data[data[\"stage\"] == 2]\n",
    "    cc = data[data[\"stage\"] == 3]\n",
    "    dd = data[data[\"stage\"] == 4]\n",
    "    a = s.mean(aa[\"gene\"])\n",
    "    b = s.mean(bb[\"gene\"])\n",
    "    c = s.mean(cc[\"gene\"])\n",
    "    d = s.mean(dd[\"gene\"])\n",
    "    f, p = stats.f_oneway(aa[\"gene\"],\n",
    "                bb[\"gene\"],\n",
    "                cc[\"gene\"], dd[\"gene\"])\n",
    "    group_indicator= pd.DataFrame(X) < np.median(pd.DataFrame(X))\n",
    "    chisq, pvalue = compare_survival(y_dfs, group_indicator)\n",
    "    if (a <= b <= c <= d) | (a >= b >= c >= d):\n",
    "        if p <= 0.05:\n",
    "            if pvalue <= 0.05:\n",
    "                pd.DataFrame(results_train_train).to_csv(\n",
    "                    path + \"/\" +str(feature_name)+\"_train.csv\", index=False)\n",
    "                pd.DataFrame(results_train_test).to_csv(\n",
    "                    path+ \"/\"+str(feature_name)+\"_test.csv\", index=False)\n",
    "                pd.DataFrame(results_train_valid).to_csv(\n",
    "                    path+ \"/\"+str(feature_name)+\"_validation.csv\", index=False)\n",
    "            else: print(feature_name, \" Kmp not satisfied\")\n",
    "        else: print(feature_name, \" Anv violin plot not satisfied\")\n",
    "    else: print(feature_name, \" Vp not satisfied\")\n",
    "    print(feature_name)\n",
    "# Get the combination of features all\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cc51a9",
   "metadata": {
    "papermill": {
     "duration": 0.014881,
     "end_time": "2022-06-19T10:41:53.717105",
     "exception": false,
     "start_time": "2022-06-19T10:41:53.702224",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    ">>>*Performance*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4a6af4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T10:41:53.749491Z",
     "iopub.status.busy": "2022-06-19T10:41:53.748749Z",
     "iopub.status.idle": "2022-06-19T10:41:53.757733Z",
     "shell.execute_reply": "2022-06-19T10:41:53.757023Z"
    },
    "papermill": {
     "duration": 0.027588,
     "end_time": "2022-06-19T10:41:53.759789",
     "exception": false,
     "start_time": "2022-06-19T10:41:53.732201",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def performance(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "    accuracy = (tp+tn)/(tn+fp+fn+tp)\n",
    "    sensitivity = tp/(tp+fn)\n",
    "    specificity = tn/(tn+fp)\n",
    "    PPV = tp/(tp+fp)\n",
    "    NPV = tn/(tn+fn)\n",
    "    f1 = metrics.f1_score(y, y_pred)\n",
    "    kappa = metrics.cohen_kappa_score(y, y_pred)\n",
    "    try:\n",
    "        # Calculate area under curve (AUC)\n",
    "        y_pred_proba = model.predict_proba(X)[::, 1]\n",
    "        auc = metrics.roc_auc_score(y, y_pred_proba)\n",
    "    except:\n",
    "        auc = \"NA\"\n",
    "    return {\"accuracy\": accuracy, \"sensitivity\": sensitivity, \"specificity\": specificity, \"auc\": auc, \"PPV\": PPV, \"NPV\": NPV, \"f1\": f1, \"kappa\": kappa}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f0d907",
   "metadata": {
    "papermill": {
     "duration": 0.01475,
     "end_time": "2022-06-19T10:41:53.789501",
     "exception": false,
     "start_time": "2022-06-19T10:41:53.774751",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    ">>>*Data Prepare Function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba64b0e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T10:41:53.821732Z",
     "iopub.status.busy": "2022-06-19T10:41:53.821004Z",
     "iopub.status.idle": "2022-06-19T10:41:53.834016Z",
     "shell.execute_reply": "2022-06-19T10:41:53.833261Z"
    },
    "papermill": {
     "duration": 0.031853,
     "end_time": "2022-06-19T10:41:53.836397",
     "exception": false,
     "start_time": "2022-06-19T10:41:53.804544",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def data_prepare(train_test, validation, num_folds, selected_features, test_size, method):\n",
    "    FeatureName = selected_features\n",
    "    models = []\n",
    "    dt = np.dtype([('DFS', np.bool_), ('DFSM', np.float64)])\n",
    "    y_dfs = np.array([],dtype=dt)\n",
    "    for i in tqdm(range(0, validation.shape[0])):\n",
    "        y_dfs = np.append(y_dfs, np.array([(validation.loc[validation.index[i], 'DFS'],\n",
    "                        validation.loc[validation.index[i], 'DFSM'])], dtype=dt))\n",
    "    for i in tqdm(FeatureName, desc = \"Processing...\"):\n",
    "        feature_name = i.split(\"-\")\n",
    "        selected_features = []\n",
    "        for k in feature_name:\n",
    "            selected_features.append(k)\n",
    "        selected_features.append(\"target\")\n",
    "#         print(\"Processing on:\", i)\n",
    "        data1 = train_test[selected_features]\n",
    "        data2 = validation[selected_features]\n",
    "        # Splitting the data\n",
    "        y = data1[\"target\"]\n",
    "        X = data1.drop([\"target\"], axis=1)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "        # Transformation\n",
    "        if method == \"Standard\":\n",
    "            scaler = StandardScaler()\n",
    "        else:\n",
    "            scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "        X_test = scaler.transform(X_test)\n",
    "        # Validation\n",
    "        X_valid = data2.drop([\"target\"], axis=1)\n",
    "        X_valid = scaler.fit_transform(X_valid)\n",
    "        y_valid = data2[\"target\"]\n",
    "        y_valid2 = validation[\"stage\"]\n",
    "        \n",
    "        models.append([X_train, X_test, X_valid, y_train,\n",
    "                      y_test, y_valid, num_folds, i, y_valid2, y_dfs])\n",
    "    return models\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de05a477",
   "metadata": {
    "papermill": {
     "duration": 0.014884,
     "end_time": "2022-06-19T10:41:53.866333",
     "exception": false,
     "start_time": "2022-06-19T10:41:53.851449",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    ">>>*Multiprocess Function*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2990ef1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T10:41:53.898727Z",
     "iopub.status.busy": "2022-06-19T10:41:53.898002Z",
     "iopub.status.idle": "2022-06-19T10:41:53.903214Z",
     "shell.execute_reply": "2022-06-19T10:41:53.902609Z"
    },
    "papermill": {
     "duration": 0.023781,
     "end_time": "2022-06-19T10:41:53.905176",
     "exception": false,
     "start_time": "2022-06-19T10:41:53.881395",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parallel(value, output, num_workers):\n",
    "    tokenizer = Vectorizer(worker_func=parameter_tuning, num_workers=num_workers, worker_init=None, callback_func=None)\n",
    "    return tokenizer.process(value, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc8234f",
   "metadata": {
    "papermill": {
     "duration": 0.014971,
     "end_time": "2022-06-19T10:41:53.935133",
     "exception": false,
     "start_time": "2022-06-19T10:41:53.920162",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# .Data Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906a778f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T10:41:53.967793Z",
     "iopub.status.busy": "2022-06-19T10:41:53.966956Z",
     "iopub.status.idle": "2022-06-19T10:41:54.186129Z",
     "shell.execute_reply": "2022-06-19T10:41:54.185387Z"
    },
    "papermill": {
     "duration": 0.238511,
     "end_time": "2022-06-19T10:41:54.188780",
     "exception": false,
     "start_time": "2022-06-19T10:41:53.950269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Create a list of features\n",
    "import os\n",
    "file = os.listdir(\"/kaggle/working/feature\")\n",
    "file\n",
    "featurelist = open(\"/kaggle/working/feature/featurelist.txt\", \"r\").read().split(\".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eab21ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T10:41:54.220836Z",
     "iopub.status.busy": "2022-06-19T10:41:54.220426Z",
     "iopub.status.idle": "2022-06-19T10:41:54.245354Z",
     "shell.execute_reply": "2022-06-19T10:41:54.244163Z"
    },
    "papermill": {
     "duration": 0.04371,
     "end_time": "2022-06-19T10:41:54.247685",
     "exception": false,
     "start_time": "2022-06-19T10:41:54.203975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Feature Function\n",
    "def getfeature(featurelist, used, numberoffeature, mode = \"used\"):\n",
    "    minus = [item for item in featurelist if item not in used]\n",
    "    try: \n",
    "        selected_features = random.sample(minus, numberoffeature)\n",
    "    except: \n",
    "        if len(minus) <= numberoffeature:\n",
    "            selected_features = minus\n",
    "    used.update(selected_features)\n",
    "    with open(\"/kaggle/working/used/\" + ml + \"used.txt\", 'w') as f:\n",
    "        f.write(\".\".join(used))\n",
    "    message = str(len(featurelist)-len(used)) + \" features remaining\"\n",
    "    while True:\n",
    "            try:\n",
    "                !git -C \"/kaggle/working/used\" add {ml}{mode}.txt\n",
    "                !git -C \"/kaggle/working/used\" commit -m \"$message\"\n",
    "                !git -C \"/kaggle/working/used\" stash\n",
    "                !git -C \"/kaggle/working/used\" pull --rebase\n",
    "                !git -C \"/kaggle/working/used\" push origin master\n",
    "                if mode == \"used\":\n",
    "                    print(\"Uploaded used\")\n",
    "                else: print(\"Uploaded done\")\n",
    "                break\n",
    "                \n",
    "            except:\n",
    "                !git -C \"/kaggle/working/used\" stash\n",
    "                print(\"Problem uploading used\")\n",
    "                continue\n",
    "    \n",
    "    return(selected_features, used)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c592e6d8",
   "metadata": {
    "papermill": {
     "duration": 0.015339,
     "end_time": "2022-06-19T10:41:54.278184",
     "exception": false,
     "start_time": "2022-06-19T10:41:54.262845",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf03272",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-19T10:41:54.394657Z",
     "iopub.status.busy": "2022-06-19T10:41:54.393745Z",
     "iopub.status.idle": "2022-06-19T10:46:20.698417Z",
     "shell.execute_reply": "2022-06-19T10:46:20.696590Z"
    },
    "papermill": {
     "duration": 266.323944,
     "end_time": "2022-06-19T10:46:20.701168",
     "exception": false,
     "start_time": "2022-06-19T10:41:54.377224",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "used = set()\n",
    "while True:\n",
    "    print(\"\\nUPDATING FEATURE...\\n\")\n",
    "    !git -C \"/kaggle/working/used\" stash\n",
    "    !git -C \"/kaggle/working/used\" pull --rebase \n",
    "    for i in range(1,11):\n",
    "        used.update(open(\"/kaggle/working/used/ml\"+str(i)+\"used.txt\", \"r\").read().split(\".\"))\n",
    "    done = set(open(\"/kaggle/working/used/\" + ml + \"done.txt\", \"r\").read().split(\".\"))\n",
    "    print(\"\\nPREPARING DATA...\\n\")\n",
    "    if len(featurelist) != len(used):\n",
    "        selected_features, used = getfeature(featurelist, used, 5000)\n",
    "        # 02. Prepare data\n",
    "        all_inputs = data_prepare(\n",
    "            train, validation,  number_of_folds, selected_features, test_size, method)\n",
    "\n",
    "        # Machine learning\n",
    "        print(\"\\nSTART MACHINE LEARNING...\\n\")\n",
    "        mp.set_start_method('fork', force = True)\n",
    "        if __name__ == '__main__':\n",
    "            parallel(\n",
    "                all_inputs, path, -1)\n",
    "        while True:\n",
    "            try:\n",
    "                !git -C \"/kaggle/working/output\" add -A\n",
    "                !git -C \"/kaggle/working/output\" commit -m \"add results {ml}\"\n",
    "                !git -C \"/kaggle/working/output\" stash\n",
    "                !git -C \"/kaggle/working/output\" pull --rebase\n",
    "                !git -C \"/kaggle/working/output\" push origin master\n",
    "                print(\"Uploaded output\")\n",
    "                break\n",
    "                \n",
    "            except:\n",
    "                !git -C \"/kaggle/working/output\" stash\n",
    "                print(\"Problem uploading output\")\n",
    "                continue\n",
    "        \n",
    "        print(\"Done\")\n",
    "        done.update(selected_features)\n",
    "        with open(\"/kaggle/working/used/\" + ml + \"done.txt\", 'w') as f:\n",
    "            f.write(\".\".join(done))\n",
    "        while True:\n",
    "            try:\n",
    "                !git -C \"/kaggle/working/used\" add {ml}done.txt\n",
    "                !git -C \"/kaggle/working/used\" commit -m \"update {ml}\"\n",
    "                !git -C \"/kaggle/working/used\" stash\n",
    "                !git -C \"/kaggle/working/used\" pull --rebase\n",
    "                !git -C \"/kaggle/working/used\" push origin master\n",
    "                print(\"Uploaded done\")\n",
    "                break\n",
    "                \n",
    "            except:\n",
    "                !git -C \"/kaggle/working/used\" stash\n",
    "                print(\"Problem uploading done\")\n",
    "                continue\n",
    "        print(\"Again\")\n",
    "        continue\n",
    "    else: \n",
    "        for i in range(1,11):\n",
    "            done.update(open(\"/kaggle/working/used/ml\"+str(i)+\"done.txt\", \"r\").read().split(\".\"))\n",
    "        print(\"\\nThere are \", str(len(done)), \" features remaining\\n\")\n",
    "        sleep(5)\n",
    "        if len(featurelist) != len(done):\n",
    "            selected_features, done = getfeature(featurelist, done, 5000, mode = \"done\")\n",
    "        # 02. Prepare data\n",
    "        all_inputs = data_prepare(\n",
    "            train, validation,  number_of_folds, selected_features, test_size, method)\n",
    "\n",
    "        # Machine learning\n",
    "        print(\"\\nSTART MACHINE LEARNING...\\n\")\n",
    "        mp.set_start_method('fork', force = True)\n",
    "        if __name__ == '__main__':\n",
    "            parallel(\n",
    "                all_inputs, path, -1)\n",
    "        while True:\n",
    "            try:\n",
    "                !git -C \"/kaggle/working/output\" add -A\n",
    "                !git -C \"/kaggle/working/output\" commit -m \"add results {ml}\"\n",
    "                !git -C \"/kaggle/working/output\" stash\n",
    "                !git -C \"/kaggle/working/output\" pull --rebase\n",
    "                !git -C \"/kaggle/working/output\" push origin master\n",
    "                print(\"Uploaded output\")\n",
    "                break\n",
    "                \n",
    "            except:\n",
    "                !git -C \"/kaggle/working/output\" stash\n",
    "                print(\"Problem uploading output\")\n",
    "                continue\n",
    "        \n",
    "        print(\"YOU'VE DONE YOUR THESISSSSSSSSSSSSSSSSSSSSSSSSSSSSS\")\n",
    "        break\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 369.238745,
   "end_time": "2022-06-19T10:46:23.779508",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-06-19T10:40:14.540763",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
